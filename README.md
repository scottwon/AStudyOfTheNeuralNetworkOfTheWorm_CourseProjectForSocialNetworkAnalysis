# A Study of the Neural Network of Caenorhabditis Elegans Worm
This is the course project for "Models and Algorithms for Social Network Analysis". I accomplished this project with my teammates, Weixiao Wang and Hao Chen.

We have learned that the scale-free networks are ubiquitous. Scale-free networks obey the power law distribution. BA network is a kind of typical scale-free networks which can be generated by the iteration of two fundamental mechanisms: 1. new nodes are joining into the network constantly; 2. new nodes are more likely to connect to the existing nodes that have a larger degree.

I examined the degree distribution of the neural network of the worm. It turned out that the degree distribution of the neural network did not obey the power law distribution. In the paper, I tried to answer the question why did the degree distribution of the neural network not obey the power law distribution.

My hypothesis is that the major difference between the BA network and the neural network is that few new neurons will be generated and incoporated into the neural network (although more recent discoveries have revealed that new neurons will be generated in hippocampus through the lifespan, which is called "neuro-genesis" and plays a vital part in people's cognitive health) while new nodes are generated and incoporated into the BA network constantly. As a matter of fact, neurons that have few connections tend to be eliminated by the 'pruning' process following the principle of 'use it or lose it' and the total number of neurons tend to decline after early adulthood. As new nodes are incorporated into the network constantly, there are a vast number of nodes that have few connections in the BA network. In contrast, neurons that have few connections tend to be eliminated, so they are relatively sparse in the neural network. I think this explains why the degree distribution of the neural network not obey the power law distribution.

I proposed 3 underlying mechanisms to generate the neural networks:

1. All neurons are created equal, that is, the neurons are randomly connected with each other initially.

2. The rewiring process is ongoing throughout the lifetime. The neurons that have the most connections tend to gain new connections, while the neurons that have the least connections tend to lose connections.

3. The pruning process, which means that the least connected neurons tend to be eliminated.

I tested whether a network that share the same degree distribution with the neural network will be generated by the iteration of the abovementioned mechanisms and the result is partially positive.

I also examined whether the neural network will maintain its structural stability during the process of rewiring and pruning.
